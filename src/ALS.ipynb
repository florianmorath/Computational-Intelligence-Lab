{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares \n",
    "\n",
    "baseline algorithm using surprise package:\n",
    "\n",
    "using `baseline_only.BaselineOnly` algorithm from http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory behind\n",
    "\n",
    "Rating of user u and item i is estimated based on the following equation: \n",
    "\n",
    "$r_{ui} = \\mu + b_u + b_i$\n",
    "\n",
    "where $\\mu$ is the overall average rating, $b_u$ and $b_i$ are biases that capture tendencies of users to rate higher/lower and tendencies of items to be rated higher/lower.\n",
    "\n",
    "The regularized square loss over which we optimize looks as follows:\n",
    "\n",
    "$\\sum_{r_{ui} \\in R_{train}}(r_{ui}-(\\mu + b_u + b_i))^2 + \\lambda (b_u^2 + b_i^2)$\n",
    "\n",
    "The optimization method used to minimize the loss function above is ALS which alternatively updates $b_u$ and $b_i$ to convexify the loss function. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handler\n",
    "from surprise_extensions import CustomReader, get_ratings_from_predictions\n",
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "We load the data using our custom reader.\n",
    "See: http://surprise.readthedocs.io/en/stable/getting_started.html#use-a-custom-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CustomReader()\n",
    "filepath = data_handler.get_train_file_path()\n",
    "data = Dataset.load_from_file(filepath, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation\n",
    "We run cross-validation with the built in function `cross_validate` using default 5-fold cv.\n",
    "\n",
    "This gives us default error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Evaluating RMSE, MAE of algorithm BaselineOnly on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0009  0.9986  1.0000  0.9984  0.9987  0.9993  0.0010  \n",
      "MAE (testset)     0.8060  0.8044  0.8062  0.8042  0.8043  0.8050  0.0009  \n",
      "Fit time          3.07    3.56    3.35    3.58    3.39    3.39    0.18    \n",
      "Test time         3.62    3.27    3.56    3.35    3.52    3.46    0.13    \n"
     ]
    }
   ],
   "source": [
    "from surprise import BaselineOnly\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "algo = BaselineOnly()\n",
    "# do a cross validation with 5 folds\n",
    "results = cross_validate(algo, data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "We load the test data to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=36, iid=0, r_ui=3.0, est=3.2514233714031837, details={'was_impossible': False})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = data_handler.get_test_file_path()\n",
    "test_data = Dataset.load_from_file(test_file_path, reader=reader)\n",
    "testset = test_data.construct_testset(test_data.raw_ratings)\n",
    "predictions = algo.test(testset)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the predictions into the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = get_ratings_from_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Id,Predict'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = data_handler.write_submission(ratings, 'submission_surprise_baseline.csv')\n",
    "print(output[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
