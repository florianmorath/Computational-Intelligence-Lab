{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN using scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from surprise_helpers import CustomReader, get_ratings_from_predictions\n",
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "We load the data using our custom reader.\n",
    "See: http://surprise.readthedocs.io/en/stable/getting_started.html#use-a-custom-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CustomReader()\n",
    "filepath = helpers.get_train_file_path()\n",
    "data = Dataset.load_from_file(filepath, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search\n",
    "We search for good values of parameters of the chosen algorithm.\n",
    "\n",
    "First we need to define the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "from surprise.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "algos = [\n",
    "    (KNNBasic, 'KNNBasic'),\n",
    "    (KNNWithMeans, 'KNNBasic'),\n",
    "    (KNNWithZScore, 'KNNWithZScore'),\n",
    "    (KNNBaseline, 'KNNBaseline')\n",
    "]   \n",
    "param_grid = {\n",
    "    'k': stats.randint(10,80),\n",
    "    'min_k': stats.randint(1, 9),\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'msd', 'pearson', 'pearson_baseline'],\n",
    "        'user_based': [False, True],\n",
    "        'min_support': [1, 10],\n",
    "        'shrinkage': [0, 100]\n",
    "    },\n",
    "    'bsl_options': {\n",
    "        'method': ['als'],\n",
    "        'n_epochs': stats.randint(10, 15),\n",
    "        'reg_i': stats.randint(8, 12),\n",
    "        'reg_u': stats.randint(12, 18)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through all algo types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_store(algos):\n",
    "    best_algos = []\n",
    "    for algo, algo_name in algos:\n",
    "        rs = RandomizedSearchCV(algo,\n",
    "                                param_grid,\n",
    "                                n_iter=25,\n",
    "                                measures=['rmse'], cv=5, n_jobs=-1,\n",
    "                                refit=True # so we can use test() directly\n",
    "                                )\n",
    "\n",
    "        rs.fit(data)\n",
    "        timestamp = time.now()\n",
    "        print('Best score {} with parameters:'.format(rs.best_score['rmse']))\n",
    "        best_params_df = pd.DataFrame.from_dict(rs.best_params['rmse'])\n",
    "        best_params_df.to_pickle('{}_best_params_{}.pkl'.format(algo_name))\n",
    "        results_df = pd.DataFrame.from_dict(rs.cv_results)\n",
    "        results_df.to_pickle('{}_results_{}.pkl')\n",
    "        res.append((rs, algoname, timestamp))\n",
    "    return best_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algos = fit_and_store(algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "We load the test data to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=36, iid=0, r_ui=3.0, est=3.1668749043413382, details={'actual_k': 35, 'was_impossible': False})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = helpers.get_test_file_path()\n",
    "test_data = Dataset.load_from_file(test_file_path, reader=reader)\n",
    "testset = test_data.construct_testset(test_data.raw_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the predictions for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_write(best_algos):\n",
    "    for rs, algo_name, timestamp in best_algos:\n",
    "        predictions = rs.test(testset)\n",
    "        # We need to convert the predictions into the right format.\n",
    "        ratings = get_ratings_from_predictions(predictions)\n",
    "        file_name = 'submission_{}_{}.csv'.format(algo_name, timestamp)\n",
    "        # Now we can write the file.\n",
    "        output = helpers.write_submission(ratings, file_name)\n",
    "        print('Wrote predictions to \"{}\"'.format(file_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_write(best_algos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
